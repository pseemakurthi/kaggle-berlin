# kaggle-berlin
Material of the Kaggle Berlin meetup group!

# Collection of Sources

If you want a comprehensive introduction to the field you find decent advice [[here]](https://80000hours.org/ai-safety-syllabus/#reading-list). Note that this is a guide for AI safety yet the areas outlined with books and sources is fairly decent.

Here is a small, but growing, collection of sources that we have been discussing on our hack sessions.

Star ratings are from :star: to :star::star::star::star::star: and subject of discussions in the Kaggle group.

## Tutorials

**[0]** Nicolas P. Rougier, Python & Numpy [[link]](http://www.labri.fr/perso/nrougier/from-python-to-numpy/) **(Outstanding Numpy introduction for scientists and optimizers)** :star::star::star::star::star:

**[1]** Sebastian Ruder, gradient descent methods [[link]](http://sebastianruder.com/optimizing-gradient-descent/) **(If you are wondering what it is all about stochastic gradient descent, Nesterov momentum, Adam, ...)** :star::star::star:

**[2]** Scikit-learn documentation [[link]](http://scikit-learn.org/stable/documentation.html) **(Absolutely great read to start learning about specific topics. Tons of superb example code. When I am bored I spend time here!)** :star::star::star::star::star:

**[3]** Donne Martin, "**Data Science iPython notebooks.**" [[Github repository]](https://github.com/donnemartin/data-science-ipython-notebooks) **(Some useful examples to learn from.)** :star::star:

## Toying and Fun (but still learning)


**[0]** Andrew Karpathy: CNNs in the browser [[link]](http://cs.stanford.edu/people/karpathy/convnetjs/) **(Great to gain some intuition.)** :star::star::star:

**[1]** Loss Function tumblr [[link]](https://lossfunctions.tumblr.com) **(If you do not suffer from PTSD from neural network training already ;))** :star::star::star::star:

**[2]** Tensorflow in the browser [[link]](https://playground.tensorflow.org) **(Start with this when you learn about NNs!)** :star::star::star::star:

**[3]** Narayanan, Arvind; Shmatikov, Vitaly: Robust De-anonymization of Large Sparse Datasests [[paper]](https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf) **(Ridiculous example of de-anonymization - this should make you very afraid! Anonymous identities in the Netflix challenge data set are discovered via public available data on IMDB.)**

**[4]** IBM personality insights [[link]](https://personality-insights-livedemo.mybluemix.net) **(Maps text to big five personality traits with Twitter or free text input. Supports English, Spanish, Arabic, and Japanese.)** :star::star::star::star:

**[5]** Visualizing the DBScan algorithm [[link]](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) **(Underrated clustering algorithm, only K-means and DBScan are useful bread-and-butter clustering algorithms.)** :star::star::star::star:


## Practical Tips


**[0]** Aarshay Jain: Complete Guide to Parameter Tuning in XGBoost (with codes in Python) [[link]](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) **(XGBoost won many Kaggle competitions and is from the gradient boosted tree-based model family.)**

**[1]** HJ van Veen: Feature Engineering [[slideshare]](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750) **(Read this to understand basics of preprocessing and feature engineering!)** :star::star::star::star:

**[2]** hat y: Kaggle Ensembling Guide [[link]](http://mlwave.com/kaggle-ensembling-guide/) **(You must learn on how to combine several submission files and stack several models together if you want to score highly in contests.)** :star::star::star::star:

**[3]** Megan Risdal: Communicating Data Science [[kaggle blog]](http://blog.kaggle.com/tag/communicating-data-science/) **(Communication of your results is one of the major skills you have to learn - and you can exercise it in our group! It is a good summary of communication, presentation, and visualization.)** :star::star::star:

**[4]** Tim Dettmers: Which GPU(s) to Get for Deep Learning. [[article]](http://timdettmers.com/2017/03/19/which-gpu-for-deep-learning/) **(Excellent guide on how to build your GPU machine, what to look for, and why cloud is too expensive)** :star::star::star::star:

## Books

**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. "**Deep learning**." An MIT Press book. (2015). [[pdf]](https://github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf) **(Good theory book to get started, modern! Then go to papers.)** :star::star::star::star:

**[1]** Murphy, Kevin. "**Machine Learning**" An MIT Press book. (2012) [[link]](https://mitpress.mit.edu/books/machine-learning-0) **(Not a good starter book, comprehensive and mathematics heavy. I use this a reference manual)** :star::star::star:

**[2]** Bishop, Christopher. "**Pattern Recognition and Machine Learning**" Springer. (2008) [[link]](http://www.springer.com/de/book/9780387310732) **(Written like a typical CS book, a bit outdated but solid introduction.)** :star::star::star:

**[3]** Abu-Mostafa, Yaser "**Learning From Data**" AMLBook (2012) [[class site]](http://www.springer.com/de/book/9780387310732) **(If you have only two months to learn ML, also has an accompanied class at Caltech.)** :star::star::star:
